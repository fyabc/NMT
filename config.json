{
    /// Basic config.

    /// Job name, must set it!
    "name": null,

    // Reload the previous saved config? Default is True.
    // If False, it will use current config and save it (will overwrite previous config)
    // If there is not any previous config, this config is useless: just load current config.
    "reload_config": true,

    // Float type.
    "floatX": "float32",
    // Random seed.
    "seed": 1234,

    // Logging filename.
    // directory is '${PROJECT_ROOT}/log/${job_name}/'
    "logging_file": "log.txt",

    // Append logging file or create a new logging file?
    "append": false,

    /// Train type, useless now
    // Candidates:
    //     baseline
    //     (More in future)
    "type": "baseline",

    /// Model name
    // directory is '${PROJECT_ROOT}/model/${job_name}'
    "model_file": "model.npz",

    // Reload model of ? iteration. Default is None.
    // If this is None, will load newest model, and this will be set to the iteration of this model.
    // If this <= 0, will restart the training, and old models will be overwritten.
    // If this > 0, will load the model of this iteration.
    "start_iteration": null,

    // Open Theano profile mode?
    "profile": false,

    /// Data paths.
    // '~' means '${PROJECT_ROOT}/data'

    // Training data filenames
    "data_src": "~/train/edit_dis_top1M.en",
    "data_tgt": "~/train/edit_dis_top1M.fr",

    // Vocabulary filenames
    "vocab_src": "~/dic/en2fr_en_vocabs_top1M.pkl",
    "vocab_tgt": "~/dic/en2fr_fr_vocabs_top1M.pkl",

    /// Hyper-parameters.

    // Vocabulary size
    "n_words_src": 30000,
    "n_words_tgt": 30000,

    // Discount learning rate after ? iterations
    "lr_discount_freq": 80000,

    "batch_size": 64,
    "valid_batch_size": 80,
    "maxlen": 1000,         // Maximum length of the description

    // Max iteration, a very large number, finish after this many updates
    "max_iteration": 1000000,

    /// Model structure config

    // Model type (unused now)
    "model": "Gru-FastFw",

    "to_upper_layer": "FastFw",

    // Number of layers in encoder/decoder
    "n_encoder_layer": 4,
    "n_decoder_layer": 4,

    "dim_word": 100,        // Word vector dimensionality
    "dim": 1000,            // The number of LSTM units
    "alignment_dim": 512,

    // Model options
    "use_zigzag": true,
    "use_half": true,
    "use_theta": true,
    "upload_emb": false,
    "use_final_residual": false,

    // Some initialize options?
    "init_distribution": "norm",
    "init_affine_weight": 0.01

}
