{
    /// Basic config.

    /// Job name, must set it!
    "name": null,

    // Reload the previous saved config? Default is True.
    // If False, it will use current config and save it (will overwrite previous config)
    // If there is not any previous config, this config is useless: just load current config.
    "reload_config": true,

    // Float type.
    "floatX": "float32",
    // Random seed.
    "seed": 1234,

    // Logging filename.
    // directory is '${PROJECT_ROOT}/log/${job_name}/'
    "logging_file": "log.txt",

    // Append logging file or create a new logging file?
    "append": false,

    /// Train type
    "type": "baseline",

    /// Model name
    // directory is '${PROJECT_ROOT}/model/${job_name}'
    "model_file": "model.npz",

    // Reload model of ? iteration. Default is None.
    // If this is None, will load newest model, and this will be set to the iteration of this model.
    // If this <= 0, will restart the training, and old models will be overwritten.
    // If this > 0, will load the model of this iteration.
    "start_iteration": null,

    /// Data paths.
    // '~' means '${PROJECT_ROOT}/data'

    // Training data filenames
    "data_src": "~/train/edit_dis_top1M.en",
    "data_tgt": "~/train/edit_dis_top1M.fr",

    // Vocabulary filenames
    "vocab_src": "~/dic/en2fr_en_vocabs_top1M.pkl",
    "vocab_tgt": "~/dic/en2fr_fr_vocabs_top1M.pkl",

    /// Hyper-parameters.

    // Vocabulary size
    "n_words_src": 30000,
    "n_words_tgt": 30000,

    // Discount learning rate after ? iterations
    "lr_discount_freq": 80000,

    "batch_size": 128,
    "maxlen": 1000,

    // Max iteration, a very large number
    "max_iteration": 1000000
}
